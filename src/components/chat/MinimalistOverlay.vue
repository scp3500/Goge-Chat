<script setup>
import { ref, computed, onMounted, onUnmounted, watch, nextTick } from 'vue';
import { invoke, convertFileSrc } from '@tauri-apps/api/core';
import { listen, emit as tauriEmit } from '@tauri-apps/api/event';
import { useChatStore } from '../../stores/chat';
import { useConfigStore } from '../../stores/config';
import * as PIXI from 'pixi.js';
import { Live2DModel, config } from 'pixi-live2d-display';

// ğŸ¨ å¼•å…¥æç®€æ¨¡å¼ä¸“å±æ ·å¼è¡¨ (ä½ å¯ä»¥ä¿®æ”¹ assets/css/minimalist.css)
import '../../assets/css/minimalist.css';
// ğŸŸ¢ ã€å”¯ä¸€æ§åˆ¶æŒ‰é’®ã€‘ - æ”¹è¿™ä¸ªæ•°ï¼ŒAlice å°±ä¼šå˜å¤§å˜å°
// ========================================================
const SIZE = 1.25;
const ALICE_ZOOM = 1;  
const Y_OFFSET = 0.5;
// ========================================================

// æ ¸å¿ƒé…ç½® (æ ¹æ® Alice æ¨¡å‹çš„ 77 ä¸ªé®ç½©è¿›è¡Œä¿®æ­£)
// å…¨å±€æ³¨å…¥ PIXI (åœ¨ä»»ä½• Live2D æ“ä½œä¹‹å‰)
if (typeof window !== 'undefined') {
  window.PIXI = PIXI;
}

// æ ¸å¿ƒé…ç½® (æ ¹æ® Alice æ¨¡å‹çš„ 77 ä¸ªé®ç½©è¿›è¡Œä¿®æ­£)
config.cubism4.maskSize = 4096;
config.cubism4.renderTextureCount = 1; // ğŸ­ å¿…é¡»ä¸º 1ï¼Œé…åˆé­”æ”¹åº“ä½¿ç”¨
if (Live2DModel.config) {
    Live2DModel.config.maxMasks = 256; // ğŸ­ è¿›ä¸€æ­¥æé«˜ä¸Šé™ try 256
}

// æ€§èƒ½æ¨¡å¼å®Œå…¨åŒæ­¥
PIXI.Program.defaultFragmentPrecision = PIXI.PRECISION.MEDIUM;
PIXI.Ticker.shared.maxFPS = 240;
PIXI.Filter.defaultResolution = window.devicePixelRatio || 1;

const props = defineProps({
  visible: { type: Boolean, default: false }
});

const emit = defineEmits(['close', 'send']);

const chatStore = useChatStore();
const configStore = useConfigStore();
const inputText = ref('');
const inputRef = ref(null);
const isSending = ref(false);
const isRecording = ref(false);
const mediaRecorder = ref(null);
const audioChunks = ref([]);

// å­—å¹•ç›¸å…³çŠ¶æ€
const subtitleText = ref('');
const isTyping = ref(false);

// æ‹–æ‹½çª—å£ä½ç½®çŠ¶æ€
const windowPos = ref({ x: 0, y: 0 });
const isDragging = ref(false);
const dragOffset = ref({ x: 0, y: 0 });
const inputWidth = ref(1200);

// Live2D ç›¸å…³çŠ¶æ€
const live2dApp = ref(null);
const live2dModel = ref(null);
const isSpeaking = ref(false);
const currentLipValue = ref(0);
// ğŸš€ [æ ¸å¿ƒä¿®å¤]ï¼šæ¨¡å‹ä½ç½®ç‹¬ç«‹åŒ–ï¼Œä¸å†ç»‘å®šçª—å£
const modelPos = ref({ x: 0, y: 0 });
const isModelDragging = ref(false);

// ğŸ¤ TTS æµå¼æ’­æ”¾ç›¸å…³çŠ¶æ€
const audioQueue = ref([]);
const isPlayingAudio = ref(false);
const currentAudioElement = ref(null);
const accumulatedText = ref(''); // ç´¯ç§¯çš„æ–‡æœ¬,ç”¨äºå¥å­åˆ‡åˆ†

// ğŸš€ [V3] åºåˆ—åŒ–æ’­æ”¾æ§åˆ¶
const nextAssignIndex = ref(0); // ğŸš€ [é‡æ„] ä¸‹ä¸€ä¸ªè¦åˆ†é…ç»™æ–‡æ®µçš„åºå·
const nextToDeliverIndex = ref(0); // ğŸš€ [é‡æ„] æœŸæœ›å…¥é˜Ÿçš„éŸ³é¢‘åºå·
const pendingAudioMap = ref(new Map()); // å­˜å‚¨ä¹±åºåˆ°è¾¾çš„éŸ³é¢‘ {sequenceIndex: audioItem}
const sentenceBuffer = ref(''); // ğŸš€ [V3] çŸ­å¥åˆå¹¶ç¼“å†²åŒº
const MIN_SENTENCE_LENGTH = 15; // ğŸš€ [ä¼˜åŒ–] é™ä½é˜ˆå€¼ï¼Œè®©çŸ­è¯­ä¹Ÿèƒ½åŠæ—¶è§¦å‘ TTS
const MAX_FORCE_SPLIT_LENGTH = 35; // ğŸš€ [æ–°å¢] å¼ºåˆ¶åˆ‡åˆ†é•¿åº¦ï¼Œé˜²æ­¢æ— æ ‡ç‚¹é•¿å¥æ†‹æ­»

// ğŸš€ [V2] ä»»åŠ¡é”å®šä¸è¯·æ±‚ ID
const currentRequestId = ref(0);
const currentSessionToken = ref(''); 
const isWaitingForResponse = ref(false); // ğŸš€ [V2] ç”¨äºè¿‡æ»¤æ—§æ¶ˆæ¯çš„æµ

// ğŸš€ [V5] å­—èŠ‚æµæ’­æ”¾æ ¸å¿ƒçŠ¶æ€
const audioCtx = ref(null);
const nextChunkTime = ref(0);
const residualBuffer = ref(null); // ğŸš€ [ä¿®å¤] ç”¨äºå¤„ç† PCM å­—èŠ‚å¯¹é½çš„æ®‹ç•™ç¼“å†²åŒº
const currentStreamingSequence = ref(-1);
const isStreamPlaying = ref(false);

// ğŸš€ [æ€§èƒ½ç›‘æµ‹] æµ‹é‡æ‰“ç«å»¶è¿Ÿ
const perfMetrics = ref({
    sendTime: 0,
    llmFirstTokenTime: 0,
    ttsTriggerStartTime: 0, // LLM å‡ºç¬¬ä¸€ä¸ªè¯çš„æ—¶é—´
    ttsRequestStartTime: 0, // çœŸæ­£å‘ç»™ Rust åç«¯çš„æ—¶é—´
    ttsFirstAudioTime: 0,
    hasTrackedLLM: false,
    hasTrackedTTS: false
});

const initAudioContext = async () => {
    if (!audioCtx.value) {
        audioCtx.value = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 32000 });
        console.log('[TTS] AudioContext å·²åˆå§‹åŒ– (32000Hz), State:', audioCtx.value.state);
    }
    if (audioCtx.value.state === 'suspended') {
        return audioCtx.value.resume().then(() => {
            console.log('[TTS] AudioContext å·²æ¢å¤');
        });
    }
    return Promise.resolve();
};

// å¤åˆ»ç‹¬ç«‹ç‰ˆçª—å£æ‹–æ‹½
const onWindowMouseDown = (e) => {
  if (e.button === 0) {
    startDragging(e);
  }
};

const initPosition = () => {
    const width = window.innerWidth;
    const height = window.innerHeight;

    // 1. è¾“å…¥æ¡†ä½ç½®ï¼šå±…ä¸­åº•éƒ¨
    const maxWidth = Math.min(1600, width * 0.80);
    inputWidth.value = maxWidth;
    windowPos.value = {
        x: (width - maxWidth) / 2,
        y: height - 50 - 100
    };

    // 2. æ¨¡å‹åˆå§‹ä½ç½®ï¼šå±…ä¸­ (æˆ–ä½ å–œæ¬¢çš„ä»»ä½•ä½ç½®)
    modelPos.value = {
        x: width * 0.5,
        y: height * 0.5
    };
};

const updatePassthroughMonitor = async () => {
    if (!props.visible) return;
    try {
        const regions = [];
        // ğŸš€ [æ€§èƒ½æ ¸å¿ƒ] æ‹–æ‹½ä¸­åˆ¤å®šåŒºé”å®šä¸ºå…¨å±ï¼Œä¸”ä¸éšä½ç½®æ›´æ–°è§¦å‘ IPC
        if (isDragging.value || isModelDragging.value) {
            regions.push({ x: 0, y: 0, w: window.innerWidth, h: window.innerHeight });
        } else {
            // 1. è¾“å…¥æ¡†åˆ¤å®šåŒº
            regions.push({ x: windowPos.value.x, y: windowPos.value.y, w: inputWidth.value, h: 100 });
            // 2. æ¨¡å‹åˆ¤å®šåŒº
            if (live2dModel.value) {
                const model = live2dModel.value;
                regions.push({
                    x: model.x - (model.width * 0.4),
                    y: model.y - (model.height * 0.5),
                    w: model.width * 0.8,
                    h: model.height * 0.9
                });
            }
        }

        const regionsStr = JSON.stringify(regions);
        if (window._lastPassthroughRegions === regionsStr) return;
        window._lastPassthroughRegions = regionsStr;

        await invoke('start_passthrough_monitor', { regions });
    } catch (e) {
        console.error('Failed to update passthrough regions:', e);
    }
};

// æ¢æµ‹æ˜¯å¦æ­£åœ¨æ‹–æ‹½
const isAnyDragging = computed(() => isDragging.value || isModelDragging.value);

// ğŸš€ [æ€§èƒ½æ ¸å¿ƒ] ä»…ç›‘å¬çŠ¶æ€åˆ‡æ¢ï¼Œä¸ç›‘å¬å…·ä½“åæ ‡
watch([isAnyDragging, () => props.visible, live2dModel], () => {
    updatePassthroughMonitor();
}, { immediate: true });

// ç›‘å¬å…¨å±€é¼ æ ‡åæ ‡ (ä»…ç”¨äºçœ‹å‘é¼ æ ‡)
let unlistenMouseMove = null;
onMounted(async () => {
    unlistenMouseMove = await listen('global-mouse-move', (event) => {
        window.mouseX = event.payload.x;
        window.mouseY = event.payload.y;
    });
});

// ç§»åˆ°é¡¶å±‚åŒæ­¥æ³¨å†Œ
onUnmounted(() => {
    if (unlistenMouseMove) unlistenMouseMove();
});

// ç›‘å¬å¯è§æ€§
watch(() => props.visible, (newVal) => {
  if (newVal) {
    if (windowPos.value.y === 0) initPosition();
    setTimeout(() => {
        inputRef.value?.focus();
        updatePassthroughMonitor();
        initLive2D();
    }, 400);
  } else {
    invoke('set_window_ignore_cursor_events', { ignore: false });
    
    // ğŸ¤ å…³é—­ç®€çº¦æ¨¡å¼æ—¶åœæ­¢ TTS
    stopAllTTS();
    
    if (live2dApp.value) {
        console.log('[MinimalistOverlay] é”€æ¯ Live2D App');
        live2dApp.value.destroy(false, { children: true, texture: true, baseTexture: true });
        live2dApp.value = null;
        live2dModel.value = null;
    }
  }
});

// ğŸš€ [ä¿®å¤] ç§»é™¤å¯¹ live2dModel çš„ deep watchï¼Œé˜²æ­¢ Vue æ·±åº¦éå†å¤æ‚ Pixi å¯¹è±¡å¯¼è‡´é€’å½’æº¢å‡º
watch([windowPos, isDragging, isModelDragging, live2dModel], () => {
    if (props.visible) updatePassthroughMonitor();
});

// Live2D åˆå§‹åŒ–
const initLive2D = async () => {
  try {
    const canvas = document.getElementById('live2d-canvas');
    if (!canvas) return;

    const app = new PIXI.Application({
      view: canvas,
      autoStart: true,
      backgroundAlpha: 0,
      resizeTo: window,
      antialias: true,
      premultipliedAlpha: true,
      powerPreference: 'default',
      resolution: window.devicePixelRatio || 1,
      autoDensity: true,
      hello: false
    });
    live2dApp.value = app;
    app.ticker.maxFPS = 240;
    PIXI.Ticker.shared.maxFPS = 240;

    const modelUrl = '/live2d/alice/alice_model3.json';
    console.log('[MinimalistOverlay] å¼€å§‹åˆå§‹åŒ– Live2D...', modelUrl);
    const model = await Live2DModel.from(modelUrl, {
      autoInteract: true,
      idleMotionGroup: 'Idle'
    });

    if (!model) {
        console.error('[MinimalistOverlay] æ¨¡å‹åŠ è½½å¤±è´¥!');
        throw new Error('æ¨¡å‹è§£æå¤±è´¥');
    }
    console.log('[MinimalistOverlay] æ¨¡å‹åŠ è½½æˆåŠŸ');

    live2dModel.value = model;
    app.stage.addChild(model);
    model.anchor.set(0.5, 0.5);
    model.eventMode = 'static';
    
    // ğŸš€ [å…³é”®ä¿®å¤] åˆå§‹ä½ç½®åŒæ­¥
    model.x = modelPos.value.x;
    model.y = modelPos.value.y;

    // ğŸ† [æ¨¡å‹ç‹¬ç«‹æ‹–æ‹½ç›‘å¬]
    const onModelMouseDown = (e) => {
        if (e.button !== 0) {
            if (e.button === 2) isSpeaking.value = !isSpeaking.value;
            return;
        }
        
        isModelDragging.value = true;
        dragOffset.value = {
            x: e.clientX - model.x,
            y: e.clientY - model.y
        };
        
        // ğŸš€ ä½¿ç”¨åŸç”Ÿ mousemove + requestAnimationFrame ç¡®ä¿ä¸æ»‘è·Ÿæ‰‹
        let rafId = null;
        const onMove = (me) => {
            if (rafId) return;
            rafId = requestAnimationFrame(() => {
                const nx = me.clientX - dragOffset.value.x;
                const ny = me.clientY - dragOffset.value.y;
                modelPos.value = { x: nx, y: ny };
                model.x = nx;
                model.y = ny;
                rafId = null;
            });
        };
        
        const onUp = () => {
            isModelDragging.value = false;
            document.removeEventListener('mousemove', onMove);
            window.removeEventListener('mouseup', onUp);
            if (rafId) cancelAnimationFrame(rafId);
        };
        
        document.addEventListener('mousemove', onMove);
        window.addEventListener('mouseup', onUp);
    };
    canvas.addEventListener('mousedown', onModelMouseDown);

    model.internalModel.on('beforeModelUpdate', () => {
      const coreModel = model.internalModel.coreModel;
      if (coreModel) {
        const now = Date.now() / 1000;
        const set = (id, value) => coreModel.setParameterValueById(id, value, 1.0);

        set('Param66', 1.0); set('Param61', 0.0); set('Param62', 1.0);
        const stickX = Math.sin(now * 3) * 0.2;
        const stickY = Math.sin(now * 4) * 0.2;
        set('LeftStickX', stickX); set('RightStickX', -stickX);
        set('LeftStickY', stickY); set('RightStickY', stickY);
        set('ParamBreath', Math.sin(now * 1.5) * 0.5 + 0.5);

        const light = Math.sin(now * 0.8) * 0.5 + 0.5;
        set('light', light); set('Param65', light);

        if (isSpeaking.value) {
          const speed = 8;
          const noise = Math.sin(now * speed) * Math.sin(now * speed * 0.5);
          let targetOpenness = (noise + 1) / 2 * 0.8 + 0.2;
          if (Math.random() > 0.95) targetOpenness = 0;
          currentLipValue.value += (targetOpenness - currentLipValue.value) * 0.1;
        } else {
          currentLipValue.value += (0 - currentLipValue.value) * 0.1;
        }
        set('ParamMouthOpenY', currentLipValue.value);

        if (window.mouseX !== undefined && !isModelDragging.value) {
             model.focus(window.mouseX, window.mouseY);
        }
      }
    });

    const updateLayout = () => {
      app.renderer.resize(window.innerWidth, window.innerHeight);
      
      // ä½¿ç”¨ç‹¬ç«‹çš„æ¨¡å‹ä½ç½®åæ ‡
      model.x = modelPos.value.x;
      model.y = modelPos.value.y;

      const coreModel = model.internalModel.coreModel;
      let s = 1.0;
      if (coreModel && coreModel.canvasHeight) {
        s = (window.innerHeight / coreModel.canvasHeight) * ALICE_ZOOM;
      } else {
        model.scale.set(1);
        s = (window.innerHeight / model.height) * ALICE_ZOOM;
      }
      model.scale.set(s);
    };

    updateLayout();
    // ğŸš€ [å…³é”®ä¿®å¤] åŠ è½½å®Œæˆåç«‹å³åˆ·æ–°ä¸€æ¬¡ç‚¹å‡»åˆ¤å®šåŒº
    updatePassthroughMonitor();
    
    app.renderer.on('destroy', () => {
        canvas.removeEventListener('mousedown', onModelMouseDown);
    });

  } catch (e) {
    console.error('æ¸²æŸ“å¼‚å¸¸:', e);
  }
};

// ç›‘å¬å™¨å¼•ç”¨(ç”¨äºæ¸…ç†)
let unlistenMessage = null;
let unlistenTyping = null;
let unlistenStreaming = null;
let unlistenAudioChunk = null;

onMounted(async () => {
  initPosition(); 
  // ç§»é™¤çª—å£è‡ªåŠ¨é‡ç½®ï¼Œä¿ç•™ç”¨æˆ·æœ€åçš„ä½ç½®
  // window.addEventListener('resize', initPosition);
  
  // ä»…ç›‘å¬ AI çš„æ¶ˆæ¯æ¥æ›´æ–°å­—å¹•æ˜¾ç¤º
  unlistenMessage = await listen('new-social-message', (event) => {
    if (props.visible && event.payload.role === 'assistant') {
      subtitleText.value = event.payload.content;
    }
  });

  unlistenTyping = await listen('typing-status', (event) => {
    if (props.visible) {
      isTyping.value = event.payload.isTyping;
      // åªæœ‰åœ¨æ²¡æœ‰æµå¼å†…å®¹æ—¶ï¼Œæ‰æ˜¾ç¤ºâ€œæ­£åœ¨è¾“å…¥â€
      if (isTyping.value && !subtitleText.value) subtitleText.value = 'æ­£åœ¨è¾“å…¥...';
      if (!isTyping.value && subtitleText.value === 'æ­£åœ¨è¾“å…¥...') subtitleText.value = '';
    }
  });

  // ğŸš€ [æµå¼ä¼ è¾“ç›‘å¬ + TTS ç”Ÿæˆ]
  unlistenStreaming = await listen('social-streaming-chunk', (event) => {
    if (props.visible) {
      // ğŸš€ [V2 æ ¸å¿ƒä¿®å¤] å¦‚æœæˆ‘ä»¬åˆšåˆšå‘é€äº†æ¶ˆæ¯ï¼Œä½†åœ¨ç­‰ç¬¬ä¸€ä¸ª chunk æ—¶æ”¶åˆ°äº†æ—§ chunkï¼Œåˆ™ä¸¢å¼ƒ
      if (isWaitingForResponse.value && !event.payload.isFirst) {
        console.log('ğŸš® [TTS] ä¸¢å¼ƒä¸Šä¸€æ¡æ¶ˆæ¯çš„å»¶è¿Ÿ Chunk');
        return;
      }

      if (event.payload.isFirst) {
        // ğŸš€ [æ€§èƒ½ç›‘æµ‹] è®°å½• LLM é¦–å­—è€—æ—¶
        if (!perfMetrics.value.hasTrackedLLM) {
            perfMetrics.value.llmFirstTokenTime = performance.now();
            const latency = perfMetrics.value.llmFirstTokenTime - perfMetrics.value.sendTime;
            console.log(`%c[PERF] âš¡ LLM é¦–å­—å»¶è¿Ÿ: ${latency.toFixed(2)}ms`, "color: #00ff00; font-weight: bold;");
            perfMetrics.value.hasTrackedLLM = true;
        }
        isWaitingForResponse.value = false; // æ”¶åˆ°ç¬¬ä¸€ä¸ª chunkï¼Œæ­£å¼å¼€å§‹
        subtitleText.value = event.payload.content;
        accumulatedText.value = event.payload.content;
      } else {
        subtitleText.value += event.payload.content;
        accumulatedText.value += event.payload.content;
      }
      
      // ğŸš€ [V3] å¦‚æœæµå¼ä¼ è¾“ç»“æŸ,å¼ºåˆ¶åˆ·æ–°ç¼“å†²åŒº
      if (event.payload.isDone) {
        console.log('[TTS] æµå¼ä¼ è¾“ç»“æŸ,åˆ·æ–°ç¼“å†²åŒº');
        flushSentenceBuffer();
      } else {
        // ğŸ¤ æ£€æµ‹å¥å­ç»“æŸ,å°è¯•åˆå¹¶æˆ–è§¦å‘ TTS ç”Ÿæˆ
        checkAndGenerateTTS();
      }
    }
  });

  // ğŸš€ [V5] æ¥æ”¶å­—èŠ‚æµ Chunk å¹¶è°ƒåº¦æ’­æ”¾
  unlistenAudioChunk = await listen('tts-audio-chunk', (event) => {
    const { requestId, sequenceId, chunk, isDone, backendPrepMs } = event.payload;
    
    // ğŸš€ [æ€§èƒ½ç›‘æµ‹] è®°å½• TTS é¦–å£°è€—æ—¶
    if (!perfMetrics.value.hasTrackedTTS && chunk && sequenceId === 0) {
        perfMetrics.value.ttsFirstAudioTime = performance.now();
        const totalLatency = perfMetrics.value.ttsFirstAudioTime - perfMetrics.value.sendTime;
        const llmWait = perfMetrics.value.llmFirstTokenTime - perfMetrics.value.sendTime;
        const ttsSentenceWait = perfMetrics.value.ttsRequestStartTime - perfMetrics.value.llmFirstTokenTime;
        const ttsFullWait = perfMetrics.value.ttsFirstAudioTime - perfMetrics.value.ttsRequestStartTime;
        
        console.log(`%c[PERF-ANALYSIS] ğŸ› ï¸ å…¨é“¾è·¯å»¶è¿Ÿåˆ†æ (æ€»è®¡: ${totalLatency.toFixed(0)}ms)`, "background: #222; color: #ffeb3b; padding: 4px; border-radius: 4px; font-weight: bold;");
        console.log(`  1. LLM æ‰“ç« (é—®->ç­”): ${llmWait.toFixed(0)}ms`);
        console.log(`  2. TTS ç¼“å†²ç­‰å¾… (ç­”->é€): ${ttsSentenceWait.toFixed(0)}ms (ç­‰å¾…æ ‡ç‚¹ç¬¦å·)`);
        console.log(`  3. TTS åç«¯å¤„ç† (é€->å“): ${ttsFullWait.toFixed(0)}ms (å«ç½‘ç»œ+æ¨ç†)`);
        if (backendPrepMs) {
            console.log(`     â””â”€ å…¶ä¸­ GPT-SoVITS æ¨ç†è€—æ—¶: ${backendPrepMs}ms`);
        }
        perfMetrics.value.hasTrackedTTS = true;
    }

    // æ‰¾åˆ°å¯¹åº”çš„éŸ³é¢‘é¡¹ (ä¼˜å…ˆä»ä¸»é˜Ÿåˆ—æ‰¾ï¼Œæ‰¾ä¸åˆ°å†å»æš‚å­˜åŒºæ‰¾)
    let audioItem = audioQueue.value.find(i => Number(i.sequenceIndex) === Number(sequenceId));
    if (!audioItem) {
        audioItem = pendingAudioMap.value.get(Number(sequenceId));
    }

    if (audioItem && audioItem.audioData === 'STREAMING') {
        if (chunk) {
            let raw = base64ToArrayBuffer(chunk);
            
            // ğŸš€ [ä¿®å¤] å¦‚æœæ˜¯ç¬¬ä¸€æ³¢æ•°æ®ä¸”åŒ…å« RIFF (WAV) å¤´ï¼Œè£æ‰å‰ 44 å­—èŠ‚
            if (audioItem.chunks.length === 0 && raw.byteLength >= 44) {
                const u8 = new Uint8Array(raw);
                if (u8[0] === 0x52 && u8[1] === 0x49 && u8[2] === 0x46 && u8[3] === 0x46) {
                    console.log('[V5-CHUNK] æ£€æµ‹åˆ° WAV å®¹å™¨å¤´ï¼Œå·²è‡ªåŠ¨å‰¥ç¦» PCM æ•°æ®');
                    raw = raw.slice(44);
                }
            }

            const isMatch = isPlayingAudio.value && Number(currentStreamingSequence.value) === Number(sequenceId);
            
            if (isMatch) {
                schedulePCMChunk(raw);
            } else {
                audioItem.chunks.push(raw);
            }
        }
        if (isDone) {
            audioItem.isDone = true;
            console.log(`[V5] åºåˆ— ${sequenceId} æµç»“æŸ`);
        }
    }
  });
});

onUnmounted(() => {
  window.removeEventListener('resize', initPosition);
  if (unlistenMessage) unlistenMessage();
  if (unlistenTyping) unlistenTyping();
  if (unlistenStreaming) unlistenStreaming();
  
  // æ¸…ç† Live2D èµ„æº
  if (live2dApp.value) {
    live2dApp.value.destroy(true, { children: true, texture: true });
    live2dApp.value = null;
  }
});

// å¤„ç†è¾“å…¥æ¡†é€»è¾‘
const handleKeyDown = (e) => {
  if (e.key === 'Enter' && !e.shiftKey) {
    e.preventDefault();
    handleSend();
  } else if (e.key === 'Escape') {
    emit('close');
  } else if (e.key === 'v' && e.altKey) {
    e.preventDefault();
    if (isRecording.value) {
      stopRecording();
    } else {
      startRecording();
    }
  }
};

const handleSend = async () => {
  const text = inputText.value.trim();
  if (!text || isSending.value) return;

  // ğŸš€ [æ€§èƒ½ç›‘æµ‹] è®°å½•å¼€å§‹å‘é€æ—¶é—´
  perfMetrics.value.sendTime = performance.now();
  perfMetrics.value.hasTrackedLLM = false;
  perfMetrics.value.hasTrackedTTS = false;
  
  // ğŸš€ æ¸…ç©ºä¹‹å‰çš„å­—å¹•å†…å®¹ï¼Œå‡†å¤‡æ¥æ”¶æ–°æµ
  subtitleText.value = '';
  
  // ğŸ¤ æ¸…ç©º TTS é˜Ÿåˆ—å’Œç´¯ç§¯æ–‡æœ¬
  stopAllTTS();
  accumulatedText.value = '';
  
  // ğŸš€ [V3] é‡ç½®åºåˆ—ç´¢å¼•ä¸ç¼“å†²åŒº
  nextAssignIndex.value = 0;
  nextToDeliverIndex.value = 0;
  pendingAudioMap.value.clear();
  sentenceBuffer.value = '';
  
  // ğŸš€ [V2] äº§ç”Ÿæ–°çš„è¯·æ±‚ ID,è®©æ—§çš„åå°ä»»åŠ¡å¤±æ•ˆ
  try {
    currentRequestId.value = await invoke('next_tts_request_id');
    currentSessionToken.value = Math.random().toString(36).substring(7);
    isWaitingForResponse.value = true; // å¼€å§‹ç­‰å¾…æ–°çš„ä¸€è½®
  } catch(e) {
    console.warn('è·å–è¯·æ±‚ ID å¤±è´¥:', e);
  }
  
  try {
    isSending.value = true;
    
    // 1. å‘é€æ¶ˆæ¯åˆ°åç«¯å¹¶è·å–çœŸæ­£çš„æ•°æ®åº“ ID
    const msgId = await invoke('save_social_message', {
      contactId: chatStore.activeSocialContactId,
      sessionId: chatStore.activeSocialSessionId,
      role: 'user',
      content: text,
      fileMetadata: null
    });

    // 2. å…¨å±€åŒæ­¥ï¼šç¡®ä¿å­—æ®µä¸ SocialChatContainer.vue é‡Œçš„ç›‘å¬å™¨å®Œå…¨åŒ¹é…
    const msgData = {
        messageId: msgId, // å¿…é¡»åŒ…å« IDï¼Œå¦åˆ™ä¸»ç•Œé¢ä¼šå› ä¸ºæ‰¾ä¸åˆ° ID è€ŒåŒæ­¥å¤±è´¥
        contactId: chatStore.activeSocialContactId,
        sessionId: chatStore.activeSocialSessionId,
        role: 'user',
        content: text,
        createdAt: new Date().toISOString() // è¿™é‡Œçš„å˜é‡åä¹Ÿè¦æ³¨æ„
    };

    // å‘å°„ç»™çˆ¶ç»„ä»¶
    emit('send', msgData);
    
    // å…³é”®ä¿®å¤ï¼šå‘å°„å…¨å±€äº‹ä»¶ï¼Œç¡®ä¿å­—æ®µåç¬¦åˆ SocialChatContainer çš„è§£æ„é€»è¾‘
    await tauriEmit('new-social-message', {
        ...msgData,
        created_at: msgData.createdAt // ä¸»ç•Œé¢ç”¨çš„æ˜¯ created_at
    });

    // 3. è§¦å‘æ²‰æµ¸å¼å›å¤é€»è¾‘
    if (configStore.settings.chatMode?.enabled) {
      // ğŸš€ [æ€§èƒ½ç›‘æµ‹] è®°å½•å¼€å§‹è¯·æ±‚ AI çš„æ—¶é—´
      perfMetrics.value.ttsRequestStartTime = performance.now(); // åˆæ­¥è®°å½•ï¼Œä¼šè¢«åé¢çš„è¦†ç›–
      
      await invoke('send_social_message_immersive', {
        sessionId: chatStore.activeSocialSessionId,
        contactId: chatStore.activeSocialContactId,
        content: text
      });
    }

    // 4. æ¸…ç©ºè¾“å…¥æ¡†å¹¶ç¡®ä¿ç„¦ç‚¹ä¿æŒ
    inputText.value = '';
    await nextTick();
    inputRef.value?.focus();
  } catch (e) {
    console.error('Failed to send:', e);
  } finally {
    isSending.value = false;
  }
};

// è¯­éŸ³å½•åˆ¶é€»è¾‘
const startRecording = async () => {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    audioChunks.value = [];
    
    mediaRecorder.value = new MediaRecorder(stream);
    mediaRecorder.value.ondataavailable = (event) => {
      if (event.data.size > 0) {
        audioChunks.value.push(event.data);
      }
    };
    
    mediaRecorder.value.onstop = async () => {
      const audioBlob = new Blob(audioChunks.value, { type: 'audio/webm' });
      await processAudio(audioBlob);
      stream.getTracks().forEach(track => track.stop());
    };
    
    mediaRecorder.value.start();
    isRecording.value = true;
    console.log('[MinimalistOverlay] å¼€å§‹å½•éŸ³');
  } catch (e) {
    console.error('å½•éŸ³å¤±è´¥:', e);
  }
};

const stopRecording = () => {
  if (mediaRecorder.value && isRecording.value) {
    mediaRecorder.value.stop();
    isRecording.value = false;
    console.log('[MinimalistOverlay] åœæ­¢å½•éŸ³');
  }
};

const processAudio = async (audioBlob) => {
  try {
    const arrayBuffer = await audioBlob.arrayBuffer();
    const audioContext = new AudioContext({ sampleRate: 16000 });
    const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
    
    // è½¬æ¢ä¸ºå•å£°é“ PCM
    const pcmData = audioBuffer.getChannelData(0);
    const samples = Array.from(pcmData);
    
    console.log('[MinimalistOverlay] å‘é€éŸ³é¢‘åˆ°ASR, æ ·æœ¬æ•°:', samples.length);
    
    const text = await invoke('transcribe_pcm', {
      samples,
      sampleRate: 16000
    });
    
    if (text && text.trim()) {
      inputText.value = text;
      await nextTick();
      inputRef.value?.focus();
    }
  } catch (e) {
    console.error('éŸ³é¢‘å¤„ç†å¤±è´¥:', e);
  }
};

// ğŸš€ [æ€§èƒ½ä¼˜åŒ–] çª—å£æ‹–æ‹½åŒæ ·ä½¿ç”¨åŸç”Ÿ + RAF
const startDragging = (e) => {
  isDragging.value = true;
  dragOffset.value = {
    x: e.clientX - windowPos.value.x,
    y: e.clientY - windowPos.value.y
  };
  
  let rafId = null;
  const onMove = (me) => {
    if (rafId) return;
    rafId = requestAnimationFrame(() => {
      windowPos.value = {
        x: me.clientX - dragOffset.value.x,
        y: me.clientY - dragOffset.value.y
      };
      rafId = null;
    });
  };
  
  const stopDragging = () => {
    isDragging.value = false;
    document.removeEventListener('mousemove', onMove);
    document.removeEventListener('mouseup', stopDragging);
    if (rafId) cancelAnimationFrame(rafId);
  };
  
  document.addEventListener('mousemove', onMove);
  document.addEventListener('mouseup', stopDragging);
};

// ğŸš€ [V3] å¼ºåˆ¶åˆ·æ–°å¹¶å‘é€ç¼“å†²åŒºä¸­çš„æ–‡æœ¬
const flushSentenceBuffer = () => {
    // å¤„ç† accumulatedText ä¸­å‰©ä½™çš„æ–‡æœ¬(ä¸ä¸€å®šå¸¦æ ‡ç‚¹)
    if (accumulatedText.value.trim()) {
        sentenceBuffer.value += accumulatedText.value.trim();
        accumulatedText.value = '';
    }
    
    if (sentenceBuffer.value.trim()) {
        console.log('[TTS] å¼ºåˆ¶åˆ·æ–°ç¼“å†²åŒº:', sentenceBuffer.value);
        const sequenceIndex = nextAssignIndex.value;
        nextAssignIndex.value++;
        generateTTSForSentence(sentenceBuffer.value.trim(), sequenceIndex);
        sentenceBuffer.value = '';
    }
};

// ğŸ¤ [TTS åŠŸèƒ½] æ£€æµ‹å¥å­ç»“æŸå¹¶ç”Ÿæˆ TTS
const checkAndGenerateTTS = () => {
  let text = accumulatedText.value;
  // ğŸš€ [é‡æ„] åŠ å…¥é€—å·æ”¯æŒï¼ŒAI è¯»åˆ°ä¸€ä¸ªçŸ­å¥æˆ–é€—å·å°±å‘èµ·è¯·æ±‚ï¼Œæ˜¾è‘—é™ä½å‰å‡ ç§’çš„ç­‰å¾…æ„Ÿ
  const sentenceEndRegex = /[ã€‚ï¼ï¼Ÿ!?ï¼›ï¼Œ, \n]/;
  
  let match;
  while ((match = text.match(sentenceEndRegex)) !== null) {
    const mark = match[0];
    const endIndex = text.indexOf(mark) + 1;
    const sentence = text.substring(0, endIndex).trim();
    
    if (sentence.length > 0) {
      sentenceBuffer.value += sentence;
      
      // ğŸš€ [é€»è¾‘ä¼˜åŒ–] åªè¦æœ‰å†…å®¹å°±ç´¯ç§¯ï¼Œé‡åˆ°æ ‡ç‚¹æˆ–é•¿åº¦è¶³å¤Ÿå°±å¼€å§‹ TTS
      if (sentenceBuffer.value.length >= MIN_SENTENCE_LENGTH || /[ã€‚ï¼ï¼Ÿ!?ï¼›,ï¼Œ\n]/.test(mark)) {
          console.log('[TTS] è§¦å‘è¯·æ±‚ (Len: ' + sentenceBuffer.value.length + ', Mark: ' + mark + '):', sentenceBuffer.value);
          const sequenceIndex = nextAssignIndex.value;
          nextAssignIndex.value++;
          generateTTSForSentence(sentenceBuffer.value, sequenceIndex);
          sentenceBuffer.value = '';
      } else {
          console.log('[TTS] å·²ç¼“å†²ç‰‡æ®µ, å½“å‰æ€»é•¿:', sentenceBuffer.value.length);
      }
    }
    
    text = text.substring(endIndex);
  }
  
  
  // ğŸš€ [æ–°å¢] å¼ºåˆ¶é•¿å¥åˆ‡åˆ†é€»è¾‘ï¼šå¦‚æœå‰©ä½™æ–‡æœ¬å¤ªé•¿ä¸”æ²¡æœ‰æ ‡ç‚¹ï¼Œå¼ºåˆ¶åˆ‡å‡ºä¸€æ®µæ¥æ’­æ”¾
  if (text.length >= MAX_FORCE_SPLIT_LENGTH) {
      console.log('[TTS] è§¦å‘å¼ºåˆ¶åˆ‡åˆ† (æ— æ ‡ç‚¹é•¿å¥):', text);
      const sequenceIndex = nextAssignIndex.value;
      nextAssignIndex.value++;
      generateTTSForSentence(text, sequenceIndex);
      text = '';
  }
  
  accumulatedText.value = text;
};

// ğŸ¤ [TTS åŠŸèƒ½] ä¸ºå•ä¸ªå¥å­ç”Ÿæˆ TTS
const generateTTSForSentence = async (sentence, sequenceIndex) => {
  const taskId = currentRequestId.value;
  const sessionToken = currentSessionToken.value;

  // ğŸš€ [æ€§èƒ½ç›‘æµ‹] è®°å½•å‘åç«¯å‘èµ·è¯·æ±‚çš„æ—¶é—´ (ä»…é’ˆå¯¹ç¬¬ä¸€æ®µ)
  if (Number(sequenceIndex) === 0) {
      perfMetrics.value.ttsRequestStartTime = performance.now();
  }

  try {
    console.log(`[V4-CORE] ç”³è¯·æœ¬åœ°ç”Ÿæˆ (ID: ${taskId}, Seq: ${sequenceIndex}):`, sentence);
    
    // ğŸš€ [æ ¸å¿ƒ] è°ƒç”¨åç«¯æœ¬åœ° TTS
    // ğŸš€ [å…³é”®ä¿®å¤] å…ˆåˆ›å»ºå¹¶åœ¨é˜Ÿåˆ—ä¸­é¢„ä½ï¼Œå†æ‰§è¡Œ invoke
    // å¦åˆ™ invoke é˜»å¡æœŸé—´åˆ°è¾¾çš„ chunk ä¼šå› ä¸ºæ‰¾ä¸åˆ° audioItem è€Œè¢«ä¸¢å¼ƒ
    const audioItem = {
      sentence,
      audioData: 'STREAMING', 
      taskId,
      sequenceIndex,
      chunks: [],
      isDone: false
    };

    // ğŸš€ [ä¸¥æ ¼äº¤ä»˜] ä»…åœ¨åºå·å¯¹é½æ—¶å…¥é˜Ÿ
    if (sequenceIndex === nextToDeliverIndex.value) {
      console.log(`[V5-STREAM] åºåˆ— ${sequenceIndex} å¯¹é½, é¢„å…¥é˜Ÿå¹¶å¼€å§‹ç”Ÿæˆ`);
      audioQueue.value.push(audioItem);
      nextToDeliverIndex.value++;
      
      // çº§è”å¤„ç†æš‚å­˜åŒº
      while (pendingAudioMap.value.has(nextToDeliverIndex.value)) {
        const nextItem = pendingAudioMap.value.get(nextToDeliverIndex.value);
        audioQueue.value.push(nextItem);
        pendingAudioMap.value.delete(nextToDeliverIndex.value);
        nextToDeliverIndex.value++;
      }
      
      if (!isPlayingAudio.value) {
        playNextAudio();
      }
    } else {
      console.log(`[V5-STREAM] åºåˆ— ${sequenceIndex} ä¹±åºåˆ°è¾¾, æš‚å­˜å…¥åº“`);
      pendingAudioMap.value.set(sequenceIndex, audioItem);
    }

    // ç°åœ¨æ‰§è¡Œ invokeï¼Œå®ƒä¼šé˜»å¡ç›´åˆ°åç«¯æµå¼æ¨é€ç»“æŸ
    const result = await invoke('generate_tts', { 
        text: sentence,
        requestId: taskId,
        sequenceId: sequenceIndex
    });
    
    // ğŸš€ [å…³é”®ä¿®å¤] Invoke è¿”å›æ„å‘³ç€æµå·²ç»ç»“æŸã€‚å³ä½¿æ²¡æ”¶åˆ° Chunk (æ¯”å¦‚åªæœ‰æ ‡ç‚¹)ï¼Œä¹Ÿè¦æ ‡ä¸º Doneã€‚
    audioItem.isDone = true;
    console.log(`[V5-STREAM] åºåˆ— ${sequenceIndex} Invoke è¿”å›å¹¶æ ‡è®°å®Œæ¯•: ${result}`);

  } catch (e) {
    console.error('[V5-STREAM] TTS ç”Ÿæˆè§¦å‘å¤±è´¥:', e);
    // è¿™é‡Œå¦‚æœå¤±è´¥äº†ï¼Œç†æƒ³æƒ…å†µä¸‹åº”è¯¥æŠŠé¢„å çš„ item æ ‡è®°ä¸º done ä»¥è·³è¿‡
  }
};

// ğŸš€ [V5] è®°å½•å½“å‰æ­£åœ¨æµå¼æ’­æ”¾çš„åºå·
// const currentStreamingSequence = ref(-1); // å·²ç§»åŠ¨åˆ°ä¸Šæ–¹é¡¶å±‚

// ğŸ¤ [TTS åŠŸèƒ½] æ’­æ”¾é˜Ÿåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªéŸ³é¢‘
const playNextAudio = async () => {
  // ğŸš€ [ä¿®å¤] å¦‚æœå·²ç»åœ¨æ’­æ”¾ï¼Œä¸å…è®¸å¤šæ¬¡è§¦å‘ï¼Œé˜²æ­¢é€»è¾‘å†²çª
  if (isPlayingAudio.value && currentStreamingSequence.value !== -1) {
      console.log('[V4-CORE] å·²ç»åœ¨æ’­æ”¾æµå¼éŸ³é¢‘ï¼Œè·³è¿‡è§¦å‘');
      return;
  }

  if (audioQueue.value.length === 0) {
    isPlayingAudio.value = false;
    isSpeaking.value = false;
    currentAudioElement.value = null;
    currentStreamingSequence.value = -1;
    return;
  }
  
  // å®‰å…¨æ ¡éªŒï¼šæ¸…é™¤è¿‡æœŸä»»åŠ¡
  if (audioQueue.value[0].taskId !== currentRequestId.value) {
      console.warn('[V4-CORE] æ¸…ç†è¿‡æœŸé˜Ÿåˆ—ä»»åŠ¡');
      audioQueue.value = [];
      isPlayingAudio.value = false;
      isSpeaking.value = false;
      return;
  }

  const audioItem = audioQueue.value[0]; 
  isPlayingAudio.value = true;
  
  try {
    if (audioItem.audioData === 'STREAMING') {
        console.log('[V5] å¯åŠ¨æµå¼æ’­æ”¾:', audioItem.sentence);
        
        // ğŸš€ [å…³é”®ä¿®å¤] å…ˆå‘å¸ƒçŠ¶æ€ï¼Œå Await ç¡¬ä»¶ã€‚é˜²æ­¢ Await æœŸé—´æ¥çš„ Chunk åŒ¹é…å¤±è´¥ã€‚
        currentStreamingSequence.value = Number(audioItem.sequenceIndex);
        isPlayingAudio.value = true;
        isSpeaking.value = true;

        await initAudioContext();
        
        // ğŸš€ [ä¼˜åŒ–] è¡”æ¥é€»è¾‘ï¼šå¦‚æœå½“å‰æ—¶é—´ç¦»ä¸Šä¸€æ¬¡è°ƒåº¦ç»“æŸè¿˜æ²¡è¶…è¿‡ 0.5 ç§’ï¼Œåˆ™ç»§ç»­è¿½åŠ è°ƒåº¦ï¼Œå®ç°æ— ç¼è¡”æ¥ã€‚
        const now = audioCtx.value.currentTime;
        if (nextChunkTime.value < now || nextChunkTime.value > now + 2.0) {
            console.log('[V5] é‡æ–°æ ¡å‡†éŸ³é¢‘è°ƒåº¦æ—¶é—´è½´');
            nextChunkTime.value = now + 0.05;
        } else {
            console.log(`[V5] æ²¿ç”¨ç°æœ‰æ—¶é—´è½´ï¼Œåç§»: ${(nextChunkTime.value - now).toFixed(3)}s`);
        }

        // æ’­æ”¾ç°æœ‰çš„ç¼“å­˜ chunks
        console.log(`[V5] è¡¥æ’­ç¼“å­˜ Chunks: ${audioItem.chunks.length} ä¸ª`);
        while (audioItem.chunks.length > 0) {
            schedulePCMChunk(audioItem.chunks.shift());
        }

        // æŒç»­æ£€æµ‹æ˜¯å¦ç»“æŸ
        let lastCheckTime = Date.now();
        let lastLength = -1;
        
        const checkEnd = () => {
            // ğŸš€ [å®‰å…¨è¡¥å¿] æ¶ˆè´¹å¯èƒ½é”™è¿‡çš„ chunks (é˜²æ­¢ç«æ€)
            while (audioItem.chunks.length > 0) {
                console.log('[V5] checkEnd è¡¥æ’­é”™è¿‡çš„é¡¹');
                schedulePCMChunk(audioItem.chunks.shift());
            }

            const isDone = audioItem.isDone && audioItem.chunks.length === 0;
            const now = Date.now();
            
            // ğŸš€ [å®‰å…¨é€€å‡º] å¦‚æœ 5 ç§’é’Ÿæ²¡æœ‰æ–°æ•°æ®ä¸”å·²æ ‡è®° Doneï¼Œå¼ºè¡Œç»“æŸï¼Œé˜²æ­¢å› ä¸ºé€»è¾‘ Bug å¡æ­»
            const isTimeout = audioItem.isDone && (now - lastCheckTime > 5000);
            
            if (isDone || isTimeout) {
                if (isTimeout) console.warn('[V5] æ£€æµ‹åˆ°æ’­æ”¾è¶…æ—¶ï¼Œå¼ºè¡Œåˆ‡æ¢ä¸‹ä¸€æ®µ');
                
                // ç­‰å¾…æœ€åä¸€æ®µæ’­å®Œ
                const waitTime = Math.max(0, (nextChunkTime.value - audioCtx.value.currentTime) * 1000 + 50);
                setTimeout(() => {
                    console.log('[V5] æµå¼æ®µç»“æŸï¼Œåˆ‡æ¢ä¸‹ä¸€æ®µ');
                    audioQueue.value.shift(); // æ¶ˆè´¹æ‰
                    isPlayingAudio.value = false; 
                    playNextAudio();
                }, waitTime);
            } else {
                if (audioItem.chunks.length !== lastLength) {
                    lastCheckTime = now;
                    lastLength = audioItem.chunks.length;
                }
                setTimeout(checkEnd, 100);
            }
        };
        checkEnd();

    } else {
        audioQueue.value.shift(); // æ™®é€šæ¨¡å¼å…ˆæ¶ˆè´¹
        let url;
        const isRemote = audioItem.audioData.startsWith('URL:');
        
        // ğŸš€ [ä¿®å¤] æ£€æµ‹æ˜¯å¦ä¸ºæœ¬åœ°æ–‡ä»¶è·¯å¾„
        const isLocalFile = /^[a-zA-Z]:\\/.test(audioItem.audioData) || audioItem.audioData.includes('\\') || audioItem.audioData.startsWith('/');

        if (isRemote) {
          url = audioItem.audioData.substring(4);
          console.log('[V4-CORE] å¯åŠ¨è¿œç¨‹æµæ’­æ”¾:', audioItem.sentence, url);
        } else if (isLocalFile) {
          url = convertFileSrc(audioItem.audioData);
          console.log('[V4-CORE] å¯åŠ¨æœ¬åœ°æ–‡ä»¶æ’­æ”¾:', audioItem.sentence, url);
        } else {
          const blob = base64ToBlob(audioItem.audioData, 'audio/wav');
          url = URL.createObjectURL(blob);
          console.log('[V4-CORE] å¯åŠ¨æœ¬åœ°äºŒè¿›åˆ¶æ’­æ”¾:', audioItem.sentence);
        }
        
        const audio = new Audio(url);
        audio.volume = 1.0; 
        currentAudioElement.value = audio;
        
        audio.onplay = () => {
          console.log('[V4-CORE] éŸ³é¢‘å¼€å§‹è¾“å‡ºï¼Œè§¦å‘åŠ¨å˜´åŠ¨ç”»');
          isSpeaking.value = true;
        };

        audio.onended = () => {
          console.log('[V4-CORE] æ’­æ”¾å®Œæˆ:', audioItem.sentence);
          if (!isRemote) URL.revokeObjectURL(url);
          isSpeaking.value = false;
          setTimeout(playNextAudio, 50); 
        };
        
        audio.onerror = (e) => {
          console.error('[V4-CORE] éŸ³é¢‘åŠ è½½/æ’­æ”¾å¤±è´¥:', e, url);
          if (!isRemote) URL.revokeObjectURL(url);
          isSpeaking.value = false;
          playNextAudio();
        };
        
        audio.play().catch(e => {
          console.error('[V4-CORE] è‡ªåŠ¨æ’­æ”¾è¢«æ‹¦æˆªæˆ–å¤±è´¥:', e);
          isSpeaking.value = false;
          playNextAudio();
        });
    }
  } catch (err) {
    console.error('[V4-CORE] æ’­æ”¾åˆå§‹åŒ–å¼‚å¸¸:', err);
    playNextAudio();
  }
};


// ğŸš€ [V5] Base64 è½¬ ArrayBuffer
const base64ToArrayBuffer = (base64) => {
    const binary = window.atob(base64);
    const len = binary.length;
    const bytes = new Uint8Array(len);
    for (let i = 0; i < len; i++) {
        bytes[i] = binary.charCodeAt(i);
    }
    return bytes.buffer;
};

// ğŸš€ [V5] è°ƒåº¦ PCM é‡‡æ ·æ’­æ”¾
const schedulePCMChunk = (arrayBuffer) => {
    if (!audioCtx.value) {
        console.warn('[TTS] AudioContext æœªåˆå§‹åŒ–ï¼Œæ”¾å¼ƒè°ƒåº¦ Chunk');
        return;
    }
    
    // è‡ªåŠ¨æ¢å¤ä¸Šä¸‹æ–‡ (é˜²æ­¢æŸäº›æµè§ˆå™¨ç”±äºéç‚¹å‡»è§¦å‘è€Œè¢«æŒ‚èµ·)
    if (audioCtx.value.state === 'suspended') {
        audioCtx.value.resume();
    }
    
    // ğŸš€ [ä¿®å¤æ ¸å¿ƒ] å­—èŠ‚å¯¹é½å¤„ç†
    let currentData = new Uint8Array(arrayBuffer);
    if (residualBuffer.value) {
        const combined = new Uint8Array(residualBuffer.value.length + currentData.length);
        combined.set(residualBuffer.value);
        combined.set(currentData, residualBuffer.value.length);
        currentData = combined;
        residualBuffer.value = null;
    }
    
    // å¦‚æœé•¿åº¦æ˜¯å¥‡æ•°ï¼Œä¿ç•™æœ€åä¸€ä¸ªå­—èŠ‚åˆ°ä¸‹ä¸€æ¬¡å¤„ç†
    if (currentData.length % 2 !== 0) {
        residualBuffer.value = currentData.slice(-1);
        currentData = currentData.slice(0, -1);
    }
    
    if (currentData.length === 0) return;

    // å°† Int16ï¼ˆ2å­—èŠ‚ï¼‰è½¬æ¢ä¸º Float32
    // è¯Šæ–­æ—¥å¿—ï¼šæ‰“å°å‰å‡ ä¸ªé‡‡æ ·å€¼
    const int16Array = new Int16Array(currentData.buffer, currentData.byteOffset, currentData.byteLength / 2);
    
    // ğŸš€ åŠ å¼ºæ—¥å¿—ï¼šå¦‚æœé‡‡æ ·å…¨ä¸º 0ï¼Œç‰¹åˆ«æŒ‡å‡º
    const isSilent = int16Array.every(v => v === 0);
    console.log(`[TTS] Chunk è°ƒåº¦: ${int16Array.length} samples, ç¬¬ä¸€ä¸ªé‡‡æ ·: ${int16Array[0]}${isSilent ? ' (âš ï¸ å…¨ç¨‹é™éŸ³!)' : ''}`);

    const float32Array = new Float32Array(int16Array.length);
    for (let i = 0; i < int16Array.length; i++) {
        float32Array[i] = int16Array[i] / 32768.0;
    }
    
    const audioBuffer = audioCtx.value.createBuffer(1, float32Array.length, 32000);
    audioBuffer.getChannelData(0).set(float32Array);
    
    const source = audioCtx.value.createBufferSource();
    source.buffer = audioBuffer;
    
    // ğŸš€ [ä¼˜åŒ–] æ˜¾å¼å¢åŠ  GainNode æ§åˆ¶éŸ³é‡
    const gainNode = audioCtx.value.createGain();
    gainNode.gain.value = 1.0; 
    
    source.connect(gainNode);
    gainNode.connect(audioCtx.value.destination);
    
    // å¢åŠ ä¸€ä¸ªæå°çš„å¹³æ»‘åç½®ï¼Œé˜²æ­¢ç”±äºè°ƒåº¦ç²¾åº¦å¯¼è‡´çš„çˆ†éŸ³
    const startTime = Math.max(audioCtx.value.currentTime, nextChunkTime.value);
    source.start(startTime);
    nextChunkTime.value = startTime + audioBuffer.duration;
};

// ğŸ¤ [TTS åŠŸèƒ½] åœæ­¢æ‰€æœ‰ TTS æ’­æ”¾
const stopAllTTS = () => {
  // åœæ­¢å½“å‰æ’­æ”¾
  if (currentAudioElement.value) {
    currentAudioElement.value.pause();
    currentAudioElement.value = null;
  }
  
  // æ¸…ç©ºé˜Ÿåˆ—
  audioQueue.value = [];
  isPlayingAudio.value = false;
  isSpeaking.value = false;
  
  // ğŸš€ [V3] æ¸…ç©ºåºåˆ—çŠ¶æ€
  nextAssignIndex.value = 0;
  nextToDeliverIndex.value = 0;
  pendingAudioMap.value.clear();
  sentenceBuffer.value = '';
  residualBuffer.value = null; // æ¸…ç©ºæ®‹ç•™ç¼“å†²
  currentStreamingSequence.value = -1;
  console.log('[TTS] å·²åœæ­¢æ‰€æœ‰æ’­æ”¾');
};

// ğŸ¤ [å·¥å…·å‡½æ•°] Base64 è½¬ Blob (æ¢å¤è¢«æ„å¤–åˆ é™¤çš„å‡½æ•°)
const base64ToBlob = (base64, type) => {
  const binaryString = window.atob(base64);
  const len = binaryString.length;
  const bytes = new Uint8Array(len);
  for (let i = 0; i < len; i++) {
    bytes[i] = binaryString.charCodeAt(i);
  }
  return new Blob([bytes], { type });
};
</script>

<template>
  <div v-if="visible" class="minimalist-root">
    <!-- Live2D Canvas å±‚ -->
    <canvas id="live2d-canvas" class="live2d-canvas"></canvas>
    
    <!-- å­—å¹•æ˜¾ç¤ºåŒºåŸŸ (å¤åˆ» ShowText) -->
    <div class="subtitle-area">
      <div class="subtitle-text" :class="{ 'is-typing': isTyping }">
        {{ subtitleText }}
      </div>
    </div>

    <!-- æ–‡æœ¬è¾“å…¥çª—å£ (å¤åˆ» TextWindow) -->
    <div 
        class="text-window" 
        :style="{ 
            left: windowPos.x + 'px', 
            top: windowPos.y + 'px',
            width: inputWidth + 'px',
            cursor: isDragging ? 'grabbing' : 'default'
        }"
    >
      <div class="h-layout">
        <input
          ref="inputRef"
          v-model="inputText"
          class="q-line-edit"
          placeholder="è¾“å…¥æ–‡å­—åæŒ‰å›è½¦..."
          @keydown="handleKeyDown"
          @mousedown.stop
          @click="() => console.log('[MinimalistOverlay] è¾“å…¥æ¡†è¢«ç‚¹å‡»')"
          @focus="() => console.log('[MinimalistOverlay] è¾“å…¥æ¡†è·å¾—ç„¦ç‚¹')"
          :disabled="isSending"
        />
        <div 
            class="voice-button" 
            :class="{ 'recording': isRecording }"
            @mousedown.stop="startRecording"
            @mouseup="stopRecording"
            @mouseleave="stopRecording"
            title="æŒ‰ä½è¯´è¯ (Alt+V)"
        >
          {{ isRecording ? 'ğŸ”´' : 'ğŸ¤' }}
        </div>
        <div 
            class="drag-button" 
            @mousedown.stop="onWindowMouseDown"
            title="æ‹–åŠ¨ä½ç½®"
        >
          â‹®â‹®
        </div>
      </div>
    </div>

    <!-- é€€å‡ºæç¤º -->
    <div class="exit-hint">
        æŒ‰ Esc æˆ–ç‚¹å‡»å¯¼èˆªæ æŒ‰é’®é€€å‡ºæç®€æ¨¡å¼
    </div>
  </div>
</template>


