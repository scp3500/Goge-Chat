use candle_core::{Device, Tensor};
use candle_nn::VarBuilder;
use candle_transformers::models::bert::{BertModel, Config};
use tauri::path::BaseDirectory;
use tauri::{AppHandle, Manager};
use tokenizers::Tokenizer;

pub struct EmbeddingEngine {
    model: BertModel,
    tokenizer: Tokenizer,
    device: Device,
}

impl EmbeddingEngine {
    pub fn new(app_handle: &AppHandle) -> Result<Self, String> {
        // ğŸ”¥ å¼ºåˆ¶åœ¨ CPU è¿è¡Œï¼Œé¿å…æŠ¢å  4070 æ˜¾å­˜
        let device = Device::Cpu;

        // è·å–åŠ¨æ€èµ„æºè·¯å¾„ (Tauri 2.0 æ ‡å‡†è§£æå™¨)
        let actual_dir = app_handle
            .path()
            .resolve("resources/bge-small-zh-v1.5", BaseDirectory::Resource)
            .map_err(|e| format!("æ— æ³•è§£æèµ„æºè·¯å¾„: {}", e))?;

        if !actual_dir.exists() {
            return Err(format!(
                "æ‰¾ä¸åˆ°æ¨¡å‹ç›®å½•: {:?}ã€‚è¯·è¿è¡Œä¸‹è½½è„šæœ¬æˆ–æ‰‹åŠ¨æ”¾ç½®æ¨¡å‹æ–‡ä»¶ã€‚",
                actual_dir
            ));
        }

        let config_path = actual_dir.join("config.json");
        let weights_path = actual_dir.join("model.safetensors");
        let tokenizer_path = actual_dir.join("tokenizer.json");

        let config =
            std::fs::read_to_string(config_path).map_err(|e| format!("è¯»å– config å¤±è´¥: {}", e))?;
        let config: Config =
            serde_json::from_str(&config).map_err(|e| format!("è§£æ config å¤±è´¥: {}", e))?;

        let tokenizer = Tokenizer::from_file(tokenizer_path)
            .map_err(|e| format!("åŠ è½½ tokenizer å¤±è´¥: {}", e))?;

        let vb = unsafe {
            VarBuilder::from_mmaped_safetensors(&[weights_path], candle_core::DType::F32, &device)
                .map_err(|e| format!("åŠ è½½æƒå€¼å¤±è´¥: {}", e))?
        };

        let model = BertModel::load(vb, &config).map_err(|e| format!("åˆå§‹åŒ– BERT å¤±è´¥: {}", e))?;

        Ok(Self {
            model,
            tokenizer,
            device,
        })
    }

    pub fn get_vector(&self, text: &str) -> Result<Vec<f32>, String> {
        let tokens = self
            .tokenizer
            .encode(text, true)
            .map_err(|e| format!("Tokenize å¤±è´¥: {}", e))?;
        let token_ids = tokens.get_ids().to_vec();
        let token_ids_tensor = Tensor::new(token_ids.as_slice(), &self.device)
            .map_err(|e| format!("åˆ›å»º Tensor å¤±è´¥: {}", e))?
            .unsqueeze(0)
            .map_err(|e| format!("Unsqueeze å¤±è´¥: {}", e))?;

        // ç®€å•çš„ BERT æ¨ç†ï¼ˆå– [CLS] å‘é‡ä½œä¸º Embeddingï¼‰
        let output = self
            .model
            .forward(
                &token_ids_tensor,
                &token_ids_tensor.zeros_like().unwrap(),
                None,
            )
            .map_err(|e| format!("æ¨¡å‹æ¨ç†å¤±è´¥: {}", e))?;

        // å– [CLS] (ç´¢å¼• 0) çš„å‘é‡
        let cls_vector = output.get(0).unwrap().get(0).unwrap();

        // L2 å½’ä¸€åŒ– (L2 Normalization)
        // v_normalized = v / sqrt(sum(v_i^2))
        let norm = cls_vector
            .sqr()
            .map_err(|e| format!("Sqr å¤±è´¥: {}", e))?
            .sum_all()
            .map_err(|e| format!("Sum å¤±è´¥: {}", e))?
            .sqrt()
            .map_err(|e| format!("Sqrt å¤±è´¥: {}", e))?;

        let normalized_vector = cls_vector
            .broadcast_div(&norm)
            .map_err(|e| format!("å½’ä¸€åŒ–å¤±è´¥: {}", e))?;

        let vector: Vec<f32> = normalized_vector
            .to_vec1()
            .map_err(|e| format!("è½¬æ¢å‘é‡å¤±è´¥: {}", e))?;

        Ok(vector)
    }
}
