use crate::commands::config_cmd;
use crate::memory::processor::{get_relevant_context, MemoryState};
use crate::models::{ChatRequest, Message};
use futures_util::StreamExt;
use serde::Serialize;
use serde_json::{json, Value};
use std::sync::atomic::Ordering;
use std::sync::Arc;
use tauri::{ipc::Channel, AppHandle, Emitter, State};
use tokio::sync::RwLock;

#[tauri::command]
pub async fn ask_ai(
    app: AppHandle,
    state: State<'_, crate::GoleState>,
    memory_state: State<'_, Arc<RwLock<MemoryState>>>,
    msg: Vec<Message>,
    on_event: Channel<String>,
    temperature: Option<f32>,
    max_tokens: Option<u32>,
    stream: Option<bool>,
    // ğŸŸ¢ æ–°å¢ï¼šå…è®¸å‰ç«¯æ˜¾å¼ä¼ å…¥å½“å‰ç»˜ç”»çš„ provider å’Œ model
    explicit_provider_id: Option<String>,
    explicit_model_id: Option<String>,
    client: State<'_, reqwest::Client>,
) -> Result<(), String> {
    // --- ğŸš€ æ ¸å¿ƒä¼˜åŒ–ï¼šå¹¶è¡Œæ‰§è¡Œé¢„å¤„ç†ä»»åŠ¡ ---
    let start_total = std::time::Instant::now(); // â±ï¸ å¼€å§‹è®¡æ—¶
    let config = config_cmd::load_config(app.clone()).await?;

    // 2. ç¡®å®šå½“å‰ä½¿ç”¨çš„æ¨¡å‹å’Œæä¾›å•†
    // ä¼˜å…ˆä½¿ç”¨æ˜¾å¼ä¼ å…¥çš„å‚æ•°ï¼Œå¦‚æœæ²¡æœ‰ï¼ˆæ—§ç‰ˆå‰ç«¯ï¼‰ï¼Œåˆ™å›é€€åˆ°å…¨å±€é…ç½®
    let selected_model = explicit_model_id.unwrap_or(config.selected_model_id.clone());
    let selected_provider_id = explicit_provider_id.unwrap_or(config.default_provider_id.clone());

    // ä» providers æ•°ç»„ä¸­æ‰¾åˆ°å½“å‰é€‰ä¸­çš„æä¾›å•†é…ç½®
    let providers = config
        .providers
        .as_array()
        .ok_or("é…ç½®é”™è¯¯ï¼šæ— æ³•è¯»å–æä¾›å•†åˆ—è¡¨")?;
    let provider_config = providers
        .iter()
        .find(|p| p["id"].as_str() == Some(&selected_provider_id))
        .ok_or(format!("æ‰¾ä¸åˆ°æä¾›å•†é…ç½®: {}", selected_provider_id))?;

    let api_key = provider_config["apiKey"].as_str().unwrap_or("").to_string();
    let base_url = provider_config["baseUrl"]
        .as_str()
        .unwrap_or("https://api.deepseek.com")
        .to_string();

    if api_key.trim().is_empty() {
        return Err(format!(
            "{} çš„ API Key æœªé…ç½®ï¼Œè¯·å‰å¾€è®¾ç½®é¡µé¢å¡«å†™",
            provider_config["name"].as_str().unwrap_or("è¯¥æä¾›å•†")
        ));
    }

    let messages = msg;

    // æ£€æŸ¥æ˜¯å¦éœ€è¦å¼ºåˆ¶ä½¿ç”¨æ¨ç† (å¦‚æœç”¨æˆ·æ‰‹åŠ¨è¾“å…¥äº† [REASON] æ ‡è®°)
    let has_reason_tag = messages
        .iter()
        .any(|m| m.role == "user" && m.content.contains("[REASON]"));

    let model = if has_reason_tag {
        // å¦‚æœæœ‰æ ‡è®°ä¸”æ˜¯ DeepSeekï¼Œåˆ‡æ¢åˆ° reasoner
        if selected_provider_id == "deepseek" {
            "deepseek-reasoner".to_string()
        } else {
            selected_model
        }
    } else {
        selected_model
    };

    // --- ğŸš€ æ ¸å¿ƒä¼˜åŒ–ï¼šå¹¶è¡Œæ‰§è¡Œ[æœç´¢]å’Œ[è®°å¿†]ä»»åŠ¡ ---
    let messages_for_search = messages.clone();
    let search_instance_url = config.search_instance_url.clone();

    // æå–è®°å¿†æ£€ç´¢å‚æ•°
    let last_user_msg = messages.iter().rev().find(|m| m.role == "user");
    let query = last_user_msg.map(|m| m.content.clone()).unwrap_or_default();
    let mode = last_user_msg
        .and_then(|m| m.mode.as_deref())
        .unwrap_or("Standard")
        .to_string();
    let role_id = last_user_msg
        .and_then(|m| m.role_id.as_deref())
        .unwrap_or("default")
        .to_string();

    // åˆ›å»ºå¹¶å‘ä»»åŠ¡
    let memory_state_inner = memory_state.inner().clone();
    let enable_rag = config.enable_rag; // ğŸš€ æ£€æŸ¥å…¨å±€ RAG å¼€å…³

    let app_for_memory = app.clone(); // âœ¨ ä¸ºå†…å­˜ä»»åŠ¡å…‹éš† AppHandle
    let memory_task = async move {
        if enable_rag {
            get_relevant_context_parallel(app_for_memory, memory_state_inner, query, mode, role_id)
                .await
        } else {
            Ok(None)
        }
    };

    let search_task = handle_search_parallel(app.clone(), messages_for_search, search_instance_url);

    // å¹¶è¡Œç­‰å¾…
    let (search_res, memory_res): (Result<Vec<Message>, String>, Result<Option<String>, String>) =
        tokio::join!(search_task, memory_task);

    let pre_processing_time = start_total.elapsed();
    println!(
        "â±ï¸ [æ€§èƒ½-åˆ†æ] å‰å¤„ç†é˜¶æ®µ(æœç´¢/è®°å¿†/é…ç½®)è€—æ—¶: {}ms",
        pre_processing_time.as_millis()
    );

    // å¤„ç†æœç´¢ç»“æœ
    let mut clean_msgs = search_res?;

    // å¤„ç†è®°å¿†ç»“æœå¹¶æ³¨å…¥
    if let Ok(Some(context)) = memory_res {
        if let Some(sys_msg) = clean_msgs.iter_mut().find(|m| m.role == "system") {
            sys_msg.content = format!("{}\n\n{}", context, sys_msg.content);
        } else {
            clean_msgs.insert(
                0,
                Message {
                    id: None,
                    model: None,
                    role: "system".to_string(),
                    content: context,
                    reasoning_content: None,
                    file_metadata: None,
                    search_metadata: None,
                    provider: None,
                    mode: None,
                    role_id: None,
                },
            );
        }
    }

    let temperature =
        temperature.or_else(|| provider_config["temperature"].as_f64().map(|f| f as f32));
    let max_tokens = max_tokens.or_else(|| provider_config["maxTokens"].as_u64().map(|u| u as u32));

    // --- ğŸ§¹ æè‡´ä¼˜åŒ–ï¼šåœ¨å‘é€ç»™ AI ä¹‹å‰æŠ¹é™¤æ‰€æœ‰é€»è¾‘æ ‡è®° ---
    for m in clean_msgs.iter_mut() {
        if m.role == "user" {
            // å‰”é™¤ [REASON]
            m.content = m.content.replace("[REASON]", "");
            // å‰”é™¤ [SEARCH] (æ”¯æŒå¸¦å‚æ•°çš„æ ¼å¼ [SEARCH:provider])
            if m.content.contains("[SEARCH") {
                // ä½¿ç”¨ç®€å•çš„æ­£åˆ™æˆ–å­—ç¬¦ä¸²å¤„ç†ç§»é™¤ [SEARCH...]
                let mut start = 0;
                while let Some(s_idx) = m.content[start..].find("[SEARCH") {
                    let absolute_start = start + s_idx;
                    if let Some(e_idx) = m.content[absolute_start..].find(']') {
                        m.content
                            .replace_range(absolute_start..=absolute_start + e_idx, "");
                        // æ›¿æ¢åå­—ç¬¦ä¸²å˜çŸ­ï¼Œä»å½“å‰ä½ç½®ç»§ç»­æ‰¾
                        start = absolute_start;
                    } else {
                        break;
                    }
                }
            }
            // æœ€ç»ˆä¿®å‰ªä¸€ä¸‹é¦–å°¾ç©ºç™½
            m.content = m.content.trim().to_string();
        }
    }

    // --- â¬‡ï¸ Google Gemini Native æ”¯æŒ â¬‡ï¸ ---
    if selected_provider_id == "gemini" {
        return handle_gemini_native(
            api_key,
            base_url,
            model,
            clean_msgs,
            state,
            on_event,
            stream.unwrap_or(true),
            &client,
            start_total,
            pre_processing_time,
        )
        .await;
    }
    // --- â¬†ï¸ Google Gemini Native æ”¯æŒ â¬†ï¸ ---

    let payload = ChatRequest {
        model: model.to_string(),
        messages: clean_msgs,
        stream: stream.unwrap_or(true),
        temperature,
        max_tokens,
    };

    let disable_url_suffix = provider_config["disableUrlSuffix"]
        .as_bool()
        .unwrap_or(false);

    let url = if disable_url_suffix {
        base_url.clone()
    } else {
        let base = base_url.trim_end_matches('/');
        if base.ends_with("/chat/completions") {
            base.to_string()
        } else if base.ends_with("/v1") {
            format!("{}/chat/completions", base)
        } else {
            // ğŸ›¡ï¸ ä¿®å¤ï¼šå¦‚æœä¸åŒ…å« v1ï¼Œè‡ªåŠ¨è¡¥å…¨ /v1/chat/completionsï¼Œä¸å‰ç«¯æµ‹è¯•ä¿æŒä¸€è‡´
            // è¿™è§£å†³äº†ç±»ä¼¼ https://api.ohmygpt.com è¿™ç§ BaseURL å¯¼è‡´çš„æµ‹è¯•é€šè¿‡ä½†å¯¹è¯å¤±è´¥çš„é—®é¢˜
            format!("{}/v1/chat/completions", base)
        }
    };

    // println!("ğŸ”— æœ€ç»ˆå¯¹è¯è¯·æ±‚åœ°å€: {}", url);

    let response = client
        .post(&url)
        .header("Authorization", format!("Bearer {}", api_key))
        .json(&payload)
        .send()
        .await
        .map_err(|e| e.to_string())?;

    if !response.status().is_success() {
        let err_body = response.text().await.unwrap_or_default();
        return Err(format!("API Error: {}", err_body));
    }

    if !stream.unwrap_or(true) {
        // --- ğŸ›‘ éæµå¼å“åº”å¤„ç† ---
        let content =
            crate::ai_utils::call_ai_backend(&client, &api_key, &base_url, &payload).await?;

        on_event
            .send(format!("c:{}", content))
            .map_err(|e| e.to_string())?;

        return Ok(());
    }

    // --- ğŸŒŠ æµå¼å“åº”å¤„ç† (âš¡ï¸ æè‡´ä¼˜åŒ–ï¼š20ms å¾®åˆæ‰¹å‡å°‘ IPC é¢‘ç‡) ---
    let mut stream = response.bytes_stream();
    let mut buffer = String::new();
    let mut ttft_logged = false;

    let mut pending_content = String::new();
    let mut pending_reasoning = String::new();
    let mut last_emit = std::time::Instant::now();
    let mut emit_count = 0; // ğŸš€ å‰å‡ ä¸ªå­—ä¸åˆæ‰¹ï¼Œç«‹å³å‘é€ä»¥è·å¾—æœ€å¿«ä½“æ„Ÿé€Ÿåº¦

    while let Some(chunk) = stream.next().await {
        if state.stop_flag.load(Ordering::Relaxed) {
            break;
        }

        let chunk = chunk.map_err(|e| e.to_string())?;
        buffer.push_str(&String::from_utf8_lossy(&chunk));

        while let Some(newline_idx) = buffer.find('\n') {
            let line = buffer.drain(..=newline_idx).collect::<String>();
            let line = line.trim();

            if line.is_empty() || line == "data: [DONE]" {
                if line == "data: [DONE]" {
                    // å½»åº•ç»“æŸå‰æ¸…ç©ºç¼“å­˜
                    if !pending_content.is_empty() {
                        let _ = on_event.send(format!("c:{}", pending_content));
                    }
                    if !pending_reasoning.is_empty() {
                        let _ = on_event.send(format!("r:{}", pending_reasoning));
                    }
                    return Ok(());
                }
                continue;
            }

            if let Some(data) = line.strip_prefix("data: ") {
                if let Ok(json) = serde_json::from_str::<serde_json::Value>(data) {
                    if let Some(choices) = json["choices"].as_array() {
                        if choices.is_empty() {
                            continue;
                        }
                        let choice = &choices[0];
                        let delta = &choice["delta"];

                        if let Some(content) = delta["content"].as_str() {
                            if !ttft_logged {
                                let network_ttft = start_total.elapsed().as_millis() as i64
                                    - pre_processing_time.as_millis() as i64;
                                println!(
                                    "â±ï¸ [æ€§èƒ½] é¦–å­—æ€»å“åº”: {}ms | ç½‘ç»œç­‰å¾…: {}ms",
                                    start_total.elapsed().as_millis(),
                                    network_ttft
                                );
                                ttft_logged = true;
                            }
                            pending_content.push_str(content);
                        }

                        if let Some(reasoning) = delta["reasoning_content"]
                            .as_str()
                            .or_else(|| delta["reasoning"].as_str())
                            .or_else(|| delta["thought"].as_str())
                        {
                            pending_reasoning.push_str(reasoning);
                        }

                        // â±ï¸ åˆ¤å®šï¼šå‰ 5 æ¬¡ä¸‹å‘ç«‹å³æ‰§è¡Œ (ä¿è¯æé€Ÿ TTFT)ï¼Œåç»­åˆ‡æ¢åˆ° 20ms å‘¨æœŸæˆ– 100 å­—ç¬¦ç¼“å†²åŒº
                        if emit_count < 5
                            || last_emit.elapsed().as_millis() >= 20
                            || pending_content.len() > 100
                        {
                            if !pending_content.is_empty() {
                                let _ = on_event.send(format!("c:{}", pending_content));
                                pending_content.clear();
                                emit_count += 1;
                            }
                            if !pending_reasoning.is_empty() {
                                let _ = on_event.send(format!("r:{}", pending_reasoning));
                                pending_reasoning.clear();
                            }
                            last_emit = std::time::Instant::now();
                        }
                    } else if let Some(err) = json["error"].as_object() {
                        return Err(format!("Stream Error: {:?}", err));
                    }
                }
            }
        }
    }

    // æ‰«å°¾
    if !pending_content.is_empty() {
        let _ = on_event.send(format!("c:{}", pending_content));
    }
    if !pending_reasoning.is_empty() {
        let _ = on_event.send(format!("r:{}", pending_reasoning));
    }

    // println!("âœ… AI ç”Ÿæˆä»»åŠ¡å·²å½»åº•é‡Šæ”¾");
    Ok(())
}

// --- â¬‡ï¸ Gemini Native ç›¸å…³ç»“æ„å’Œå®ç° â¬‡ï¸ ---

#[derive(Serialize)]
struct GeminiPart {
    text: Option<String>,
}

#[derive(Serialize)]
struct GeminiContent {
    #[serde(skip_serializing_if = "Option::is_none")]
    role: Option<String>,
    parts: Vec<GeminiPart>,
}

#[derive(Serialize)]
struct GeminiRequest {
    contents: Vec<GeminiContent>,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "systemInstruction")]
    system_instruction: Option<GeminiContent>,
}

async fn handle_gemini_native(
    api_key: String,
    base_url: String,
    model: String,
    messages: Vec<Message>,
    state: State<'_, crate::GoleState>,
    on_event: Channel<String>,
    stream: bool,
    client: &reqwest::Client,
    start_total: std::time::Instant,
    pre_processing_time: std::time::Duration,
) -> Result<(), String> {
    if !stream {
        // --- ğŸ›‘ éæµå¼å¤„ç† ---
        let content = crate::ai_utils::call_gemini_backend(
            client,
            &api_key,
            &base_url,
            &model,
            messages.clone(),
        )
        .await?;

        on_event
            .send(format!("c:{}", content))
            .map_err(|e| e.to_string())?;

        return Ok(());
    }

    // --- ğŸŒŠ æµå¼å¤„ç† ---
    // 1. è½¬æ¢æ¶ˆæ¯æ ¼å¼
    let mut system_instruction = None;
    let mut contents = Vec::new();

    for m in messages {
        if m.role == "system" {
            system_instruction = Some(GeminiContent {
                role: None,
                parts: vec![GeminiPart {
                    text: Some(m.content),
                }],
            });
        } else {
            let role = if m.role == "user" { "user" } else { "model" };
            contents.push(GeminiContent {
                role: Some(role.to_string()),
                parts: vec![GeminiPart {
                    text: Some(m.content),
                }],
            });
        }
    }

    let payload = GeminiRequest {
        contents,
        system_instruction,
    };

    // 2. æ„é€  URL (æ›´åŠ é²æ£’çš„åˆ¤æ–­)
    let mode = "streamGenerateContent";
    let base = base_url.trim_end_matches('/');

    let url = if base.contains("/models/") {
        // å¦‚æœç”¨æˆ·æä¾›äº†å®Œæ•´è·¯å¾„ï¼Œåªè¡¥å…¨ key
        format!("{}?key={}", base, api_key)
    } else {
        // æ™ºèƒ½è¡¥å…¨ç‰ˆæœ¬å’Œè·¯å¾„
        let version = if base.contains("/v1") { "" } else { "/v1beta" };
        format!(
            "{}{}/models/{}:{}?key={}",
            base, version, model, mode, api_key
        )
    };

    let response = client
        .post(&url)
        .header("Content-Type", "application/json")
        .json(&payload)
        .send()
        .await
        .map_err(|e| format!("Gemini ç½‘ç»œè¯·æ±‚å¤±è´¥: {}", e))?;

    if !response.status().is_success() {
        let status = response.status();
        let err_text = response.text().await.unwrap_or_default();
        return Err(format!("Gemini API é”™è¯¯ (çŠ¶æ€ç  {}): {}", status, err_text));
    }

    // --- ğŸŒŠ æµå¼å¤„ç† ---
    let mut stream = response.bytes_stream();
    let mut buffer = String::new();
    let mut ttft_logged = false;
    let mut pending_content = String::new();
    let mut last_emit = std::time::Instant::now();
    let mut emit_count = 0;

    while let Some(chunk) = stream.next().await {
        if state.stop_flag.load(Ordering::Relaxed) {
            break;
        }

        let chunk = chunk.map_err(|e| e.to_string())?;
        buffer.push_str(&String::from_utf8_lossy(&chunk));

        // Gemini çš„ stream æ•°æ®æ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ª JSON å¯¹è±¡çš„æ•°ç»„ï¼Œæ ¼å¼å¤§è‡´ä¸º [ {...}, {...} ]
        // è¿™é‡Œå°è¯•è§£æå®Œæ•´çš„ JSON å¯¹è±¡å—
        while let Some(start_idx) = buffer.find('{') {
            if state.stop_flag.load(Ordering::Relaxed) {
                break;
            }
            let mut depth = 0;
            let mut end_idx = None;
            let bytes = buffer.as_bytes();

            for i in start_idx..bytes.len() {
                if bytes[i] == b'{' {
                    depth += 1;
                } else if bytes[i] == b'}' {
                    if depth > 0 {
                        depth -= 1;
                        if depth == 0 {
                            end_idx = Some(i);
                            break;
                        }
                    }
                }
            }

            if let Some(end) = end_idx {
                let json_str = &buffer[start_idx..=end];
                if let Ok(json) = serde_json::from_str::<Value>(json_str) {
                    // è§£æ candidates[0].content.parts[0].text
                    if let Some(parts) = json["candidates"][0]["content"]["parts"].as_array() {
                        for part in parts {
                            if let Some(text) = part["text"].as_str() {
                                if !ttft_logged {
                                    // ğŸŸ¢ ç›‘æµ‹ï¼šä»ç”¨æˆ·è¾“å…¥åˆ°æµå¼è¾“å‡ºé¦–å­—çš„æ€§èƒ½è€—æ—¶ (Gemini)
                                    let network_ttft = start_total.elapsed().as_millis() as i64
                                        - pre_processing_time.as_millis() as i64;
                                    println!(
                                        "â±ï¸ [æ€§èƒ½] é¦–å­—æ€»å“åº” (Gemini): {}ms | ç½‘ç»œç­‰å¾…: {}ms",
                                        start_total.elapsed().as_millis(),
                                        network_ttft
                                    );
                                    ttft_logged = true;
                                }
                                pending_content.push_str(text);

                                // â±ï¸ Gemini åŒæ ·é‡‡ç”¨ï¼šå‰ 5 æ¬¡æé€Ÿå“åº”ï¼Œåç»­åˆæ‰¹ç­–ç•¥
                                if emit_count < 5
                                    || last_emit.elapsed().as_millis() >= 20
                                    || pending_content.len() > 100
                                {
                                    if !pending_content.is_empty() {
                                        on_event
                                            .send(format!("c:{}", pending_content))
                                            .map_err(|e| e.to_string())?;
                                        pending_content.clear();
                                        emit_count += 1;
                                    }
                                    last_emit = std::time::Instant::now();
                                }
                            }
                        }
                    }
                }
                buffer.drain(..=end);
            } else {
                break; // ç­‰å¾…æ›´å¤šæ•°æ®
            }
        }
    }

    Ok(())
}

#[tauri::command]
pub async fn discover_models_raw(
    url: String,
    api_key: Option<String>,
    headers_map: Option<std::collections::HashMap<String, String>>,
    client: State<'_, reqwest::Client>,
) -> Result<Value, String> {
    let mut request = client.get(&url);

    if let Some(key) = api_key {
        if !key.is_empty() {
            request = request.header("Authorization", format!("Bearer {}", key));
        }
    }

    if let Some(h) = headers_map {
        for (k, v) in h {
            request = request.header(k, v);
        }
    }

    let response = request
        .send()
        .await
        .map_err(|e| format!("Network error: {}", e))?;

    if !response.status().is_success() {
        let status = response.status();
        let err_text = response.text().await.unwrap_or_default();
        return Err(format!("API Error ({}): {}", status, err_text));
    }

    let data = response
        .json::<Value>()
        .await
        .map_err(|e| format!("JSON parse error: {}", e))?;
    Ok(data)
}

// --- ğŸš€ åŠ©æ‰‹å‡½æ•°ï¼šå¹¶è¡Œå¤„ç†æœç´¢é€»è¾‘ ---
async fn handle_search_parallel(
    app: AppHandle,
    messages: Vec<Message>,
    search_instance_url: String,
) -> Result<Vec<Message>, String> {
    let mut clean_msgs = messages.clone();

    // æ£€æŸ¥æœ€åä¸€æ¡æ¶ˆæ¯æ˜¯å¦æœ‰ [SEARCH]
    if let Some(m) = clean_msgs.last_mut() {
        if m.role == "user" && m.content.contains("[SEARCH]") {
            let (original_query, provider) = if m.content.contains("[SEARCH:") {
                let start = m.content.find("[SEARCH:").unwrap();
                let end = m.content[start..].find(']').unwrap() + start;
                let provider = &m.content[start + 8..end];
                let clean = m.content.replace(&m.content[start..=end], "");
                (clean, provider.to_string())
            } else {
                (m.content.replace("[SEARCH]", ""), "all".to_string())
            };

            // å‘é€æœç´¢å¼€å§‹äº‹ä»¶
            let _ = app.emit(
                "search-status",
                json!({ "status": "searching", "query": original_query }),
            );

            match crate::commands::search::perform_search(
                &search_instance_url,
                &original_query,
                &provider,
            )
            .await
            {
                Ok(results) => {
                    // å‘é€æœç´¢ç»“æœäº‹ä»¶
                    let _ = app.emit(
                        "search-status",
                        json!({ "status": "done", "results": results }),
                    );

                    let mut context = String::from("ã€è”ç½‘æœç´¢å‚è€ƒèµ„æ–™ã€‘\n");
                    for (i, res) in results.iter().enumerate() {
                        context.push_str(&format!(
                            "{}. {}\n   é“¾æ¥: {}\n   å†…å®¹: {}\n\n",
                            i + 1,
                            res.title,
                            res.url,
                            res.snippet
                        ));
                    }

                    m.content = format!(
                        "ç”¨æˆ·åŸå§‹é—®é¢˜: {}\n\n{}\nè¯·åˆ†æä»¥ä¸Šæœç´¢ç»“æœï¼Œç»“åˆä½ çš„çŸ¥è¯†ï¼Œä¸ºç”¨æˆ·æä¾›å‡†ç¡®ä¸”æœ€æ–°çš„å›ç­”ã€‚",
                        original_query, context
                    );
                }
                Err(e) => {
                    let _ = app.emit("search-status", json!({ "status": "error", "message": e }));
                }
            }
        }
    }

    Ok(clean_msgs)
}

// --- ğŸš€ åŠ©æ‰‹å‡½æ•°ï¼šå¹¶è¡Œå¤„ç†è®°å¿†æ£€ç´¢é€»è¾‘ ---
async fn get_relevant_context_parallel(
    app: AppHandle,
    memory_state: Arc<RwLock<MemoryState>>,
    query: String,
    mode: String,
    role_id: String,
) -> Result<Option<String>, String> {
    if query.is_empty() {
        return Ok(None);
    }

    // å‘é€è®°å¿†æ£€ç´¢å¼€å§‹äº‹ä»¶
    let _ = app.emit(
        "memory-status",
        json!({ "status": "searching", "query": query }),
    );
    let start_time = std::time::Instant::now();

    // æ‰§è¡Œè®°å¿†æ£€ç´¢
    let context = get_relevant_context(memory_state, &query, &mode, &role_id).await?;

    let duration = start_time.elapsed().as_millis();

    if !context.is_empty() {
        // å‘é€è®°å¿†æ£€ç´¢å®Œæˆäº‹ä»¶
        let _ = app.emit(
            "memory-status",
            json!({ "status": "done", "duration": duration, "has_context": true }),
        );
        Ok(Some(context))
    } else {
        // å‘é€è®°å¿†æ£€ç´¢å®Œæˆäº‹ä»¶ (æ— ç»“æœ)
        let _ = app.emit(
            "memory-status",
            json!({ "status": "done", "duration": duration, "has_context": false }),
        );
        Ok(None)
    }
}

// --- ğŸš€ è¿æ¥é¢„çƒ­ï¼šåœ¨ç”¨æˆ·è¾“å…¥æ—¶æå‰å»ºç«‹è¿æ¥ ---
#[tauri::command]
pub async fn prewarm_connection(
    base_url: String,
    client: State<'_, reqwest::Client>,
) -> Result<(), String> {
    let start = std::time::Instant::now();

    // æ„é€ ä¸€ä¸ªæœ€å°çš„å¥åº·æ£€æŸ¥è¯·æ±‚ï¼ˆé€šå¸¸ /v1/models ç«¯ç‚¹ä¸éœ€è¦é‰´æƒï¼‰
    let url = if base_url.contains("generativelanguage.googleapis.com") {
        // Gemini ä½¿ç”¨ä¸åŒçš„ç«¯ç‚¹
        format!("{}/v1beta/models", base_url.trim_end_matches('/'))
    } else {
        // OpenAI å…¼å®¹ç«¯ç‚¹
        let base = base_url.trim_end_matches('/');
        if base.ends_with("/v1") {
            format!("{}/models", base)
        } else {
            format!("{}/v1/models", base)
        }
    };

    // å‘èµ·é¢„çƒ­è¯·æ±‚ï¼ˆä¸å…³å¿ƒç»“æœï¼Œåªä¸ºå»ºç«‹è¿æ¥ï¼‰
    let _ = client
        .get(&url)
        .timeout(std::time::Duration::from_secs(5))
        .send()
        .await;

    let elapsed = start.elapsed().as_millis();
    println!(
        "ğŸ”¥ [PREWARM] Connection to {} established in {}ms",
        base_url, elapsed
    );

    Ok(())
}
